# üîç Fake Job Post Prediction

> **Industry-ready ML system** for detecting fraudulent job postings using classical ML and Transformer models.  
> Built with Python, scikit-learn, XGBoost, LightGBM, BERT (Transformers), and FastAPI.

[![Python 3.10+](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## üìä Model Performance Results

All 6 models were trained and evaluated on the [HuggingFace Fake Job Posting dataset](https://huggingface.co/datasets/victor/real-or-fake-fake-jobposting-prediction) (17,880 records).  
Evaluation was performed on a held-out **15% stratified test set**.

| Model | Accuracy | Precision | Recall | F1 Score | ROC-AUC |
|---|---|---|---|---|---|
| Baseline (DummyClassifier) | 95.2% | ‚Äî | ‚Äî | ‚Äî | 0.50 |
| Logistic Regression | 96.5% | 0.59 | **0.89** | 0.71 | **0.99** |
| **Linear SVM** | **98.2%** | 0.80 | 0.83 | **0.82** | 0.98 |
| Random Forest | 97.7% | **0.99** | 0.53 | 0.69 | 0.98 |
| XGBoost | 97.8% | 0.78 | 0.77 | 0.77 | 0.98 |
| LightGBM | 98.1% | 0.86 | 0.74 | 0.79 | 0.98 |

### Key Takeaways

- **Best overall (F1):** Linear SVM ‚Äî 0.82 F1 with 98.2% accuracy
- **Best recall (catch fraud):** Logistic Regression ‚Äî 0.89 recall (misses fewest fake posts)
- **Best precision (fewest false alarms):** Random Forest ‚Äî 0.99 precision
- **Priority metric:** F1 Score and Recall ‚Äî minimizing missed fraud is critical

---

## üìÅ Project Structure

```
Fake-Job-Post-Prediction/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ huggingface_dataset/           # Cached raw dataset from HF
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.csv                      # 70% stratified train split
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ val.csv                        # 15% validation split
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test.csv                       # 15% test split
‚îÇ   ‚îî‚îÄ‚îÄ external/                          # Optional augmentation data
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_eda.ipynb                       # Exploratory Data Analysis
‚îÇ   ‚îú‚îÄ‚îÄ 02_preprocessing.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03_feature_engineering.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 04_baseline_models.ipynb
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py                          # Centralized hyperparameters & paths
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                     # HuggingFace dataset loader + local cache
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py                  # HTML/emoji/URL removal, stopwords, fraud indicators
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ split.py                       # Stratified train/val/test splitting
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ augment.py                     # SMOTE oversampling for class imbalance
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ featurize.py                   # TF-IDF + metadata ColumnTransformer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py                       # Feature utility functions
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baseline.py                    # DummyClassifier (majority class)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ml_models.py                   # Model registry: LR, SVM, RF, XGBoost, LightGBM
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transformer.py                 # BERT fine-tuning wrapper (train/predict/save/load)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py                       # Main training script (--all, --smote, --full-features)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py                    # Evaluation metrics (Accuracy, F1, ROC-AUC, PR-AUC)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ callbacks.py                   # Early stopping callback
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ inference/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict.py                     # Single + batch prediction with saved models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ explain.py                     # SHAP & LIME explainability
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py                         # FastAPI app (4 endpoints)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py                     # Pydantic request/response schemas
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ helpers.py                     # Text combination, pattern matching
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py                     # Comprehensive metric computation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py                      # Centralized logging
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ visualization/
‚îÇ       ‚îî‚îÄ‚îÄ plots.py                       # Confusion matrix, ROC, PR curves, model comparison
‚îÇ
‚îú‚îÄ‚îÄ models/                                # Saved model artifacts (.joblib)
‚îÇ   ‚îú‚îÄ‚îÄ baseline.joblib
‚îÇ   ‚îú‚îÄ‚îÄ logistic_regression.joblib
‚îÇ   ‚îú‚îÄ‚îÄ svm.joblib
‚îÇ   ‚îú‚îÄ‚îÄ random_forest.joblib
‚îÇ   ‚îú‚îÄ‚îÄ xgboost.joblib
‚îÇ   ‚îú‚îÄ‚îÄ lightgbm.joblib
‚îÇ   ‚îî‚îÄ‚îÄ comparison.csv                     # Model comparison results
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ LICENSE
```

---

## üöÄ Quick Start

### 1. Clone & Setup Virtual Environment

```bash
git clone https://github.com/your-username/Fake-Job-Post-Prediction.git
cd Fake-Job-Post-Prediction

# Create virtual environment
python -m venv venv

# Activate (Windows)
.\venv\Scripts\activate

# Activate (Linux/Mac)
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Train Models

```bash
# Train a single model
.\venv\Scripts\python src/training/train.py --model logistic_regression

# Train ALL models and generate comparison table
.\venv\Scripts\python src/training/train.py --all

# Train with SMOTE oversampling (handles class imbalance)
.\venv\Scripts\python src/training/train.py --model xgboost --smote

# Train with full features (TF-IDF + metadata + engineered features)
.\venv\Scripts\python src/training/train.py --model xgboost --full-features
```

**Available models:** `baseline`, `logistic_regression`, `svm`, `random_forest`, `xgboost`, `lightgbm`

### 3. Run Inference

```python
from src.inference.predict import Predictor

predictor = Predictor("logistic_regression")

result = predictor.predict_single(
    "Earn $5000/week from home! No experience needed. Contact us on WhatsApp."
)
print(result)
# {'prediction': 'Fraudulent', 'label': 1, 'probability_fraudulent': 0.92, ...}
```

### 4. Start the API

```bash
uvicorn src.api.app:app --reload
```

Then visit: [http://localhost:8000/docs](http://localhost:8000/docs) for interactive Swagger documentation.

---

## üåê API Endpoints

| Route | Method | Description |
|-------|--------|-------------|
| `/health` | GET | Health check ‚Äî model status |
| `/predict` | POST | Classify a single job posting |
| `/batch` | POST | Classify multiple job postings |
| `/explain` | POST | Classify + LIME feature explanation |

### Example Request

```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Marketing Intern",
    "description": "Earn money fast from home!",
    "company_profile": "",
    "requirements": "No experience needed"
  }'
```

### Example Response

```json
{
  "prediction": "Fraudulent",
  "confidence": 0.92,
  "fraudulent_score": 0.92
}
```

---

## üß† Models & Methodology

### Tier 1 ‚Äî Classical ML (TF-IDF Features)

| Model | Library | Strategy |
|-------|---------|----------|
| Logistic Regression | scikit-learn | `class_weight='balanced'`, max_iter=1000 |
| Linear SVM | scikit-learn | `class_weight='balanced'` |

### Tier 2 ‚Äî Ensemble Models (TF-IDF + Metadata)

| Model | Library | Strategy |
|-------|---------|----------|
| Random Forest | scikit-learn | 200 estimators, `class_weight='balanced'` |
| XGBoost | XGBoost | 200 estimators, `scale_pos_weight=10` |
| LightGBM | LightGBM | 200 estimators, `class_weight='balanced'` |

### Tier 3 ‚Äî Deep Learning

| Model | Library | Strategy |
|-------|---------|----------|
| BERT | Hugging Face Transformers | `bert-base-uncased`, lr=2e-5, 4 epochs, AdamW |

### Class Imbalance Handling

The dataset is highly imbalanced (~5% fraud). We address this through:
- **Class weights** ‚Äî `balanced` weighting in all classical models
- **Scale pos weight** ‚Äî XGBoost positive class weighting
- **SMOTE** ‚Äî Synthetic minority oversampling (optional via `--smote` flag)

---

## üîß Feature Engineering

### Text Features
- **TF-IDF vectors** ‚Äî up to 5,000 features, bigrams, sublinear TF
- Combined text from: `title + company_profile + description + requirements + benefits`

### Engineered Fraud Indicators

| Feature | Rationale |
|---------|-----------|
| `email_count` | Fake posts often include personal emails |
| `url_count` | External link redirection |
| `exclamation_count` | Emotional manipulation ("Earn $$$!!!") |
| `upper_ratio` | ALL CAPS usage |
| `word_count` | Unusually short or long descriptions |
| `company_profile_len` | Fake companies have short/empty profiles |

### Metadata Features (One-Hot Encoded)
- `employment_type`, `required_experience`, `required_education`, `industry`, `function`

### Boolean Features
- `telecommuting`, `has_company_logo`, `has_questions`

---

## üìà Evaluation Metrics

| Metric | Description | Priority |
|--------|-------------|----------|
| **F1 Score** | Harmonic mean of precision & recall | ‚≠ê Primary |
| **Recall** | Fraction of actual fraud detected | ‚≠ê Primary |
| **Precision** | Fraction of predicted fraud that is real | Secondary |
| **ROC-AUC** | Overall discrimination ability | Secondary |
| **PR-AUC** | Precision-Recall area under curve | Secondary |
| **Accuracy** | Overall correctness | Baseline |

> **Priority: F1 and Recall** ‚Äî In fraud detection, missing a fake job post (false negative) is worse than a false alarm.

---

## üß™ Data Pipeline

```
HuggingFace Dataset (17,880 records)
        ‚Üì
   Text Cleaning (HTML, emoji, URL, stopword removal)
        ‚Üì
   Fraud Indicator Feature Engineering
        ‚Üì
   Stratified Split (70% train / 15% val / 15% test)
        ‚Üì
   TF-IDF Vectorization + Metadata Encoding
        ‚Üì
   Model Training & Evaluation
        ‚Üì
   Model Comparison Table (models/comparison.csv)
```

---

## üê≥ Docker & Docker Compose

### Quick Start with Docker Compose

```bash
# Start the API server
docker compose up api

# Access API
curl http://localhost:8000/health
```

### Train Models Inside Docker

```bash
# Run all model training inside a container
docker compose --profile train up trainer
```

> Models and data are mounted as volumes ‚Äî trained models persist on your host machine.

### Enable Monitoring (Prometheus + Grafana)

```bash
# Start API + Prometheus + Grafana
docker compose --profile monitoring up
```

- **API:** [http://localhost:8000](http://localhost:8000)
- **Prometheus:** [http://localhost:9090](http://localhost:9090)
- **Grafana:** [http://localhost:3000](http://localhost:3000) (admin/admin)

### Standalone Docker (without Compose)

```bash
# Build image
docker build -t fake-job-api .

# Run container
docker run -p 8000:8000 -v ./models:/app/models fake-job-api

# Access API
curl http://localhost:8000/health
```

### Docker Compose Services

| Service | Port | Profile | Description |
|---------|------|---------|-------------|
| `api` | 8000 | default | FastAPI prediction server |
| `trainer` | ‚Äî | `train` | One-off model training |
| `prometheus` | 9090 | `monitoring` | Metrics collection |
| `grafana` | 3000 | `monitoring` | Dashboards |

---

## üî¨ Explainability

### LIME (Local Interpretable Model-agnostic Explanations)
- Explains individual predictions by highlighting contributing words
- Integrated into the `/explain` API endpoint

### SHAP (SHapley Additive exPlanations)
- Global feature importance for ML models
- Available via `src/inference/explain.py`

---

## üìö Tech Stack

| Category | Libraries |
|----------|-----------|
| **Data** | pandas, numpy, datasets (HuggingFace) |
| **ML** | scikit-learn, XGBoost, LightGBM, imbalanced-learn |
| **Deep Learning** | PyTorch, Transformers (HuggingFace) |
| **NLP** | NLTK, BeautifulSoup4 |
| **API** | FastAPI, Uvicorn, Pydantic |
| **Explainability** | SHAP, LIME |
| **Visualization** | Matplotlib, Seaborn |
| **Testing** | pytest, httpx |

---

## üìù Dataset

**Source:** [victor/real-or-fake-fake-jobposting-prediction](https://huggingface.co/datasets/victor/real-or-fake-fake-jobposting-prediction)

| Field | Type | Description |
|-------|------|-------------|
| `title` | text | Job title |
| `company_profile` | text | Company description |
| `description` | text | Job description |
| `requirements` | text | Job requirements |
| `benefits` | text | Job benefits |
| `telecommuting` | binary | Remote work flag |
| `has_company_logo` | binary | Logo presence |
| `has_questions` | binary | Screening questions |
| `employment_type` | categorical | Full-time, Part-time, etc. |
| `required_experience` | categorical | Entry, Mid, Senior, etc. |
| `required_education` | categorical | Bachelor's, Master's, etc. |
| `industry` | categorical | Industry sector |
| **`fraudulent`** | **binary** | **Target ‚Äî 0 (Real) / 1 (Fake)** |

---

## üóÇ Deliverables

- ‚úÖ Clean, documented codebase (20+ source files)
- ‚úÖ Reproducible training scripts with CLI arguments
- ‚úÖ 6 trained models with comparison table
- ‚úÖ Production-ready FastAPI with 4 endpoints
- ‚úÖ SHAP & LIME explainability
- ‚úÖ Dockerized deployment
- ‚úÖ Comprehensive README with results

---

## üìÑ License

This project is licensed under the MIT License ‚Äî see [LICENSE](LICENSE) for details.
